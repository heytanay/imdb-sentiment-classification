{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nltk\nimport platform\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\n\nimport torch\nimport transformers\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.cuda.amp import GradScaler\nfrom torch.utils.data import Dataset, DataLoader\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NB_EPOCHS = 5\nLR = 1e-5\nMAX_LEN = 64\nTRAIN_BS = 64\nVALID_BS = 128\nBERT_MODEL = 'bert-base-uncased'\nFILE_NAME = '../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv'\nTOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True)","metadata":{"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"class BERTDataset(Dataset):\n    def __init__(self, review, target):\n        self.review = review\n        self.target = target\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n    \n    def __len__(self):\n        return len(self.review)\n    \n    def __getitem__(self, idx):\n        review = str(self.review[idx])\n        review = ' '.join(review.split())\n        \n        inputs = self.tokenizer.encode_plus(\n            review,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True\n        )\n        \n        ids = torch.tensor(inputs['input_ids'], dtype=torch.long)\n        mask = torch.tensor(inputs['attention_mask'], dtype=torch.long)\n        token_type_ids = torch.tensor(inputs['token_type_ids'], dtype=torch.long)\n        targets = torch.tensor(self.target[idx], dtype=torch.float)\n        \n        return {\n            'ids': ids,\n            'mask': mask,\n            'token_type_ids': token_type_ids,\n            'targets': targets\n        }","metadata":{"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Functions to train the model\nclass Trainer:\n    def __init__(\n        self, \n        model, \n        optimizer, \n        scheduler, \n        train_dataloader, \n        valid_dataloader,\n        device\n    ):\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.train_data = train_dataloader\n        self.valid_data = valid_dataloader\n        self.loss_fn = nn.BCEWithLogitsLoss()\n        self.device = device\n        \n    def train_one_epoch(self):\n        prog_bar = tqdm(enumerate(self.train_data), total=len(self.train_data))\n        self.model.train()\n        for idx, inputs in prog_bar:\n            ids = inputs['ids'].to(self.device, dtype=torch.long)\n            mask = inputs['mask'].to(self.device, dtype=torch.long)\n            ttis = inputs['token_type_ids'].to(self.device, dtype=torch.long)\n            targets = inputs['targets'].to(self.device, dtype=torch.float)\n            \n            self.optimizer.zero_grad()\n            outputs = self.model(ids=ids, mask=mask, token_type_ids=ttis)            \n            \n            loss = self.loss_fn(outputs.view(-1,), targets)\n            prog_bar.set_description('loss: {:.2f}'.format(loss.item()))\n            \n            loss.backward()\n            self.optimizer.step()\n            self.scheduler.step()\n    \n    def valid_one_epoch(self):\n        prog_bar = tqdm(enumerate(self.valid_data), total=len(self.valid_data))\n        self.model.eval()\n        all_targets = []\n        all_predictions = []\n        for idx, inputs in prog_bar:\n            ids = inputs['ids'].to(self.device, dtype=torch.long)\n            mask = inputs['mask'].to(self.device, dtype=torch.long)\n            ttis = inputs['token_type_ids'].to(self.device, dtype=torch.long)\n            targets = inputs['targets'].to(self.device, dtype=torch.float)\n            \n            outputs = self.model(ids=ids, mask=mask, token_type_ids=ttis)\n            outputs = outputs.view(-1,)\n            all_targets.extend(targets.cpu().detach().numpy().tolist())\n            all_predictions.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n        \n        output_for_accuracy = all_predictions >= 0.5\n        val_accuracy = accuracy_score(all_targets, output_for_accuracy)\n        print('Validation Accuracy: {:.2f}'.format(val_accuracy))\n        \n        return val_accuracy\n    \n    def get_model(self):\n        return self.model","metadata":{"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# Model\nclass BERTModel(nn.Module):\n    def __init__(self):\n        super(BERTModel, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(BERT_MODEL)\n        self.drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n        output = self.drop(output)\n        output = self.out(output)\n        return output","metadata":{"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# Training Code\nif __name__ == '__main__':\n    if torch.cuda.is_available():\n        print(\"[INFO] Using GPU: {}\".format(torch.cuda.get_device_name()))\n        DEVICE = torch.device('cuda:0')\n    else:\n        print(\"[INFO] GPU not found. Using CPU: {}\".format(platform.processor()))\n        DEVICE = torch.device('cpu')\n    \n    data = pd.read_csv(FILE_NAME)\n    data = data.sample(frac=1).reset_index(drop=True)\n    data['sentiment'] = data['sentiment'].map({'positive': 1, 'negative': 0})\n    \n    train_data = data[:45000].sample(frac=1).reset_index(drop=True)\n    valid_data = data[45000:].sample(frac=1).reset_index(drop=True)\n    print(f\"[INFO] Training on: {train_data.shape[0]} samples\")\n    print(f\"[INFO] Validation on: {valid_data.shape[0]} samples\")\n    \n    train_set = BERTDataset(\n        review = train_data['review'].values,\n        target = train_data['sentiment'].values\n    )\n    \n    valid_set = BERTDataset(\n        review = valid_data['review'].values,\n        target = valid_data['sentiment'].values\n    )\n    \n    train = DataLoader(\n        train_set,\n        batch_size = TRAIN_BS,\n        shuffle = True,\n        num_workers=4\n    )\n    \n    valid = DataLoader(\n        valid_set,\n        batch_size = VALID_BS,\n        shuffle = False,\n        num_workers=4\n    )\n    \n    print(\"[INFO] Created Dataloaders!\")\n    \n    model = BERTModel().to(DEVICE)\n    nb_train_steps = int(len(train_data) / TRAIN_BS * NB_EPOCHS)\n    optimizer = transformers.AdamW(model.parameters(), lr=LR)\n    scheduler = transformers.get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=nb_train_steps\n    )\n    \n    trainer = Trainer(model, optimizer, scheduler, train, valid, DEVICE)\n    print(\"[INFO] Initialized Trainer and Models, Starting training...\")\n    \n    best_accuracy = 0\n    for epoch in range(1, NB_EPOCHS+1):\n        print(f\"{'='*20} EPOCH: {epoch} {'='*20}\")\n        \n        # Train for 1 epoch\n        trainer.train_one_epoch()\n        \n        # Validate for 1 epoch\n        current_accuracy = trainer.valid_one_epoch()\n        \n        if current_accuracy > best_accuracy:\n            print(f\"Saving the Model for Best Accuracy: {current_accuracy:.4f} %\")\n            torch.save(trainer.get_model().state_dict(), \"BERT_BASE_UNCASED_MODEL.pt\")\n            best_accuracy = current_accuracy\n    print(\"Model Finished Training!\")\n    print(f\"Best Accuracy was: {best_accuracy:.4f}%\")\n    print(f\"Final Accuracy was: {current_accuracy:.4f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"[INFO] Using GPU: Tesla P100-PCIE-16GB\n[INFO] Training on: 45000 samples\n[INFO] Validation on: 5000 samples\n[INFO] Created Dataloaders!\n[INFO] Initialized Trainer and Models, Starting training...\n==================== EPOCH: 1 ====================\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/704 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7928c4963ce8431798b50b74b9f7f627"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}